# -*- coding: utf-8 -*-
"""clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14K-oO4t6X7mDiLGG-cbhI0zWGnCzrR0n
"""

#  Importing required libraries / Gerekli kÃ¼tÃ¼phaneleri yÃ¼klÃ¼yoruz
import pandas as pd  # For data manipulation / Veri iÅŸleme iÃ§in
import numpy as np   # For numerical operations / SayÄ±sal iÅŸlemler iÃ§in
import matplotlib.pyplot as plt  # For plotting / Grafik Ã§izmek iÃ§in
import seaborn as sns  # For advanced visualizations / GeliÅŸmiÅŸ gÃ¶rselleÅŸtirme iÃ§in

from sklearn.preprocessing import StandardScaler  # For data normalization / Verileri Ã¶lÃ§eklendirme
from sklearn.cluster import KMeans  # K-Means clustering algorithm / K-Means kÃ¼meleme algoritmasÄ±
from sklearn.decomposition import PCA  # For dimensionality reduction / Boyut indirgeme iÃ§in

# Connecting and accessing Google Drive
from google.colab import drive
drive.mount('/content/drive')

#  Loading the dataset / Veriyi yÃ¼klÃ¼yoruz
df = pd.read_csv("/content/drive/MyDrive/data_mining_clustering/StudentsPerformance.csv")  # Replace with your path if needed / Gerekirse kendi yolunla deÄŸiÅŸtir
df.head()  # Show the first 5 rows / Ä°lk 5 satÄ±rÄ± gÃ¶ster

#  Converting categorical columns to numeric / Kategorik sÃ¼tunlarÄ± sayÄ±sala Ã§eviriyoruz
df_encoded = pd.get_dummies(df, drop_first=True)
# Converts 'Gender' and 'Geography' to numeric using one-hot encoding / Gender ve Geography sÃ¼tunlarÄ±nÄ± one-hot ile sayÄ±sala Ã§evirir
df_encoded.head()

#  Scaling the data / Veriyi Ã¶lÃ§eklendiriyoruz
scaler = StandardScaler()  # Standardizes data (mean=0, std=1) / Verileri ortalama=0, std=1 olacak ÅŸekilde normalize eder
scaled_data = scaler.fit_transform(df_encoded)  # Apply transformation / DÃ¶nÃ¼ÅŸtÃ¼rmeyi uygula

#  Finding optimal number of clusters using Elbow Method / Elbow yÃ¶ntemi ile ideal kÃ¼me sayÄ±sÄ±nÄ± bulma
wcss = []  # Within-cluster sum of squares / KÃ¼me iÃ§i kareler toplamÄ±

for i in range(1, 11):  # Trying from 1 to 10 clusters / 1'den 10 kÃ¼meye kadar deniyoruz
    kmeans = KMeans(n_clusters=i, init="k-means++", random_state=42)
    kmeans.fit(scaled_data)
    wcss.append(kmeans.inertia_)  # Inertia means WCSS / Inertia deÄŸeri WCSS demektir

# ğŸ“ˆ Plotting the Elbow graph / Elbow grafiÄŸini Ã§iziyoruz
plt.plot(range(1, 11), wcss, marker="o")
plt.title("Elbow Method / Elbow YÃ¶ntemi")
plt.xlabel("Number of Clusters / KÃ¼me SayÄ±sÄ±")
plt.ylabel("WCSS (Within-Cluster Sum of Squares)")
plt.grid(True)
plt.show()

#  Applying KMeans clustering / KMeans kÃ¼meleme algoritmasÄ±nÄ± uyguluyoruz
kmeans = KMeans(n_clusters=3, init="k-means++", random_state=42)  # You can change the cluster count / KÃ¼me sayÄ±sÄ±nÄ± deÄŸiÅŸtirebilirsin
clusters = kmeans.fit_predict(scaled_data)  # Assign each sample to a cluster / Her veri noktasÄ±na kÃ¼me atar

df["Cluster"] = clusters  # Add cluster results to original dataframe / KÃ¼me sonuÃ§larÄ±nÄ± ana veri setine ekle

#  Reducing dimensions with PCA for visualization / PCA ile veriyi gÃ¶rselleÅŸtirmek iÃ§in 2 boyuta indiriyoruz
pca = PCA(n_components=2)  # We reduce it to 2 dimensions / 2 boyuta indiriyoruz
pca_data = pca.fit_transform(scaled_data)

df_pca = pd.DataFrame(data=pca_data, columns=["PC1", "PC2"])  # Principal Components / Ana bileÅŸenler
df_pca["Cluster"] = clusters  # Cluster labels / KÃ¼me etiketlerini ekliyoruz

# ğŸ“Œ Checking cluster statistics / KÃ¼meleme sonuÃ§larÄ±nÄ±n istatistiklerine bakÄ±yoruz
df["Cluster"].value_counts()  # Number of customers in each cluster / Her kÃ¼mede kaÃ§ mÃ¼ÅŸteri var

# ğŸ“Œ Analyzing cluster characteristics / KÃ¼melerin Ã¶zelliklerini analiz ediyoruz
# Add the cluster results to the encoded dataframe for analysis
df_encoded["Cluster"] = clusters
df_encoded.groupby("Cluster").mean()  # Cluster-wise feature averages / Her kÃ¼menin ortalama deÄŸerleri

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Veriyi yÃ¼kle
df = pd.read_csv("/content/drive/MyDrive/data_mining_clustering/StudentsPerformance.csv")

# Performans sÃ¼tunlarÄ±
X = df[["math score", "reading score", "writing score"]]

# Ã–lÃ§eklendir
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# K-Means kÃ¼meleme (Ã¶rnek: 3 kÃ¼me)
kmeans = KMeans(n_clusters=3, random_state=42)
df["Cluster"] = kmeans.fit_predict(X_scaled)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

df["PC1"] = X_pca[:, 0]
df["PC2"] = X_pca[:, 1]

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df, x="PC1", y="PC2", hue="Cluster", palette="Set2")
plt.title("Ã–ÄŸrencilerin BaÅŸarÄ±ya GÃ¶re KÃ¼melenmesi")
plt.show()

demographics = ["gender", "race/ethnicity", "parental level of education", "lunch", "test preparation course"]

for col in demographics:
    plt.figure(figsize=(8, 4))
    sns.countplot(data=df, x=col, hue="Cluster")
    plt.title(f"KÃ¼melere GÃ¶re {col} DaÄŸÄ±lÄ±mÄ±")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()